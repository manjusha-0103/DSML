{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextAnalytics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPUqs6ty2jFiavqqQsWFqfy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgkcode/DSML/blob/main/TextAnalytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6GW_tmM9h2L9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "622aea51-bfb7-4d8e-806d-200e77969716"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')\n",
        "    \n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# peform tokenization\n",
        "txt = \"The Natural Language Toolkit (NLTK) is a platform used for building Python programs that work with human language data for applying in statistical natural language processing (NLP).\"\n",
        "lower_txt = txt.lower()\n",
        "tokenized_text = sent_tokenize(lower_txt)\n",
        "print(tokenized_text)\n",
        "\n",
        "word_token = word_tokenize(lower_txt)\n",
        "print(\"\\n\",word_token)\n",
        "\n"
      ],
      "metadata": {
        "id": "5CZ53P8JSG3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f41e001c-a2a4-4b0f-dc85-dce5bf70282a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the natural language toolkit (nltk) is a platform used for building python programs that work with human language data for applying in statistical natural language processing (nlp).']\n",
            "\n",
            " ['the', 'natural', 'language', 'toolkit', '(', 'nltk', ')', 'is', 'a', 'platform', 'used', 'for', 'building', 'python', 'programs', 'that', 'work', 'with', 'human', 'language', 'data', 'for', 'applying', 'in', 'statistical', 'natural', 'language', 'processing', '(', 'nlp', ')', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove punctuation and stopwords\n",
        "import string\n",
        "from nltk.corpus import stopwords \n",
        "s_punctuate = set(string.punctuation)\n",
        "print(\"\\n punctuation mark : \",s_punctuate)\n",
        "print(stopwords.words(\"english\"))\n",
        "filterdtext = []\n",
        "\n",
        "for w in word_token :\n",
        "    if w not in s_punctuate:\n",
        "        if w not in stopwords.words(\"english\"):\n",
        "            filterdtext.append(w)\n",
        "\n",
        "print(\"\\nunfiltered text : \",word_token)        \n",
        "print(\"\\nfiltered text : \",filterdtext)        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-mtqWANSmRG",
        "outputId": "95a54ff9-8c2d-449c-f2fc-e9dc07938614"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " punctuation mark :  {'^', '!', '$', '{', ':', '[', '&', '?', '=', '_', '(', '+', '#', '*', '-', '>', '.', ',', ')', ';', '/', ']', '<', '\"', '@', '%', '`', '|', '}', \"'\", '~', '\\\\'}\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "\n",
            "unfiltered text :  ['the', 'natural', 'language', 'toolkit', '(', 'nltk', ')', 'is', 'a', 'platform', 'used', 'for', 'building', 'python', 'programs', 'that', 'work', 'with', 'human', 'language', 'data', 'for', 'applying', 'in', 'statistical', 'natural', 'language', 'processing', '(', 'nlp', ')', '.']\n",
            "\n",
            "filtered text :  ['natural', 'language', 'toolkit', 'nltk', 'platform', 'used', 'building', 'python', 'programs', 'work', 'human', 'language', 'data', 'applying', 'statistical', 'natural', 'language', 'processing', 'nlp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# performong stemming \n",
        "from nltk import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "stemm_list = []\n",
        "for w in filterdtext:\n",
        "    stemm_list.append(ps.stem(w))\n",
        "\n",
        "print(\"stemmed words : \",stemm_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cQtk8taVAVq",
        "outputId": "9d7e564b-6d40-4356-b6a9-2ddb6bcdcad1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stemmed words :  ['natur', 'languag', 'toolkit', 'nltk', 'platform', 'use', 'build', 'python', 'program', 'work', 'human', 'languag', 'data', 'appli', 'statist', 'natur', 'languag', 'process', 'nlp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Lemmetization\n",
        "from nltk import WordNetLemmatizer\n",
        "wl = WordNetLemmatizer()\n",
        "lemm_list = []\n",
        "for w in filterdtext:\n",
        "    lemm_list.append(wl.lemmatize(w))\n",
        "\n",
        "print(\"lemmatized words : \",lemm_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NS8VHn0fowYL",
        "outputId": "5eaef11d-ac10-43c4-ce13-ed98a0a21a08"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lemmatized words :  ['natural', 'language', 'toolkit', 'nltk', 'platform', 'used', 'building', 'python', 'program', 'work', 'human', 'language', 'data', 'applying', 'statistical', 'natural', 'language', 'processing', 'nlp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pos tagging\n",
        "from nltk import pos_tag\n",
        "tag_list = []\n",
        "\n",
        "for w in lemm_list:\n",
        "    #print(w)\n",
        "    tag_list.append(pos_tag([w]))\n",
        "print(\"Pos tagginng list : \",tag_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzYvGcusowje",
        "outputId": "17c9ff67-d2fd-44cf-f6f5-c6e9ae673caf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pos tagginng list :  [[('natural', 'JJ')], [('language', 'NN')], [('toolkit', 'NN')], [('nltk', 'NN')], [('platform', 'NN')], [('used', 'VBN')], [('building', 'NN')], [('python', 'NN')], [('program', 'NN')], [('work', 'NN')], [('human', 'NN')], [('language', 'NN')], [('data', 'NNS')], [('applying', 'VBG')], [('statistical', 'JJ')], [('natural', 'JJ')], [('language', 'NN')], [('processing', 'NN')], [('nlp', 'NN')]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# counting ocurrences of each word\n",
        "unique_list = set(lemm_list)\n",
        "#print(unique_list)\n",
        "\n",
        "#word_count_dict = {}\n",
        "i = 0\n",
        "'''for w in unique_list:\n",
        "    word_count_dict[w] = 0\n",
        "    if w in lemm_list:\n",
        "        word_count_dict[w] += 1\n",
        "'''\n",
        "    \n",
        "\n",
        "word_count_dict = dict.fromkeys(unique_list,0)\n",
        "print(word_count_dict)\n",
        "for word in lemm_list:\n",
        "    word_count_dict[word] += 1 \n",
        "    #numOfWordsB=dict.fromkeys(uniqueWords,0)\n",
        "    #for word in bagOfWordsB:\n",
        "        #numOfWordsB[word] += 1\n",
        "\n",
        "print(word_count_dict)    \n"
      ],
      "metadata": {
        "id": "sY6KS_qryS3p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccf903e9-0ad3-4f42-8a27-0b6862899984"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'platform': 0, 'natural': 0, 'toolkit': 0, 'data': 0, 'human': 0, 'statistical': 0, 'nltk': 0, 'program': 0, 'processing': 0, 'building': 0, 'python': 0, 'applying': 0, 'nlp': 0, 'used': 0, 'language': 0, 'work': 0}\n",
            "{'platform': 1, 'natural': 2, 'toolkit': 1, 'data': 1, 'human': 1, 'statistical': 1, 'nltk': 1, 'program': 1, 'processing': 1, 'building': 1, 'python': 1, 'applying': 1, 'nlp': 1, 'used': 1, 'language': 3, 'work': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def computeTF(wordDict,bagOfWords): \n",
        "    tfDict={}\n",
        "    bagOfWordsCount = len(bagOfWords)\n",
        "    for word,count in wordDict.items():\n",
        "        #print(word,count)\n",
        "        tfDict[word] = count/float(bagOfWordsCount)\n",
        "        #computeTF(wordDict[word],bagOfWords)\n",
        "        \n",
        "    return tfDict   \n",
        "tfB = computeTF(word_count_dict,lemm_list)\n",
        "\n",
        "print(tfB)\n",
        "\n",
        "print(\"\\n\")\n",
        "'''def termfreq(document, word):\n",
        "    N = len(document)\n",
        "    occurance = len([token for token in document if token == word])\n",
        "    return occurance/N\n",
        "termfreq(lemm_list,word_count_dict)'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "_PBjIXUcmcTt",
        "outputId": "b3ddad5d-3598-4c7e-d11e-eb448dfbb308"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'platform': 0.05263157894736842, 'natural': 0.10526315789473684, 'toolkit': 0.05263157894736842, 'data': 0.05263157894736842, 'human': 0.05263157894736842, 'statistical': 0.05263157894736842, 'nltk': 0.05263157894736842, 'program': 0.05263157894736842, 'processing': 0.05263157894736842, 'building': 0.05263157894736842, 'python': 0.05263157894736842, 'applying': 0.05263157894736842, 'nlp': 0.05263157894736842, 'used': 0.05263157894736842, 'language': 0.15789473684210525, 'work': 0.05263157894736842}\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'def termfreq(document, word):\\n    N = len(document)\\n    occurance = len([token for token in document if token == word])\\n    return occurance/N\\ntermfreq(lemm_list,word_count_dict)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "t = \"The elephant sneezed at the sight of potatoes.\"\n",
        "_t = nltk.stem.SnowballStemmer(\"english\")\n",
        "#type(_t.stem())\n",
        "for i in t.split(' ') :\n",
        "    print(_t.stem(i))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY7hc3qAKjSU",
        "outputId": "71ab7560-1175-4d0d-b9d6-12db227b9f17"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the\n",
            "eleph\n",
            "sneez\n",
            "at\n",
            "the\n",
            "sight\n",
            "of\n",
            "potatoes.\n"
          ]
        }
      ]
    }
  ]
}